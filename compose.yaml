x-common:
  &common
  logging:
    driver: local
  restart: unless-stopped
  healthcheck:
    &common-healthcheck
    start_period: 30s
    interval: 30s
    retries: 5

x-airflow-common:
  &airflow-common
  <<: *common
  image: bitnami/airflow:3.0.1
  environment:
    &airflow-common-env
    AIRFLOW_DATABASE_HOST: postgres
    AIRFLOW_DATABASE_NAME: airflow
    AIRFLOW_DATABASE_USERNAME: airflow
    AIRFLOW_DATABASE_PASSWORD: ${AIRFLOW_DATABASE_PASSWORD:-airflow}
    AIRFLOW_APISERVER_HOST: airflow
    AIRFLOW_EXECUTOR: LocalExecutor
    AIRFLOW_LOAD_EXAMPLES: no
    AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-airflow}
    AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-airflow}
    AIRFLOW_FIRSTNAME: Airflow
    AIRFLOW_LASTNAME: Admin
    AIRFLOW_EMAIL: user@example.com
    FLEET_DATABASE_PASSWORD: ${FLEET_DATABASE_PASSWORD:-fleet}
    FLEET_WAREHOUSE_PASSWORD: ${FLEET_WAREHOUSE_PASSWORD:-fleet}
  volumes:
    - ./dags:/opt/bitnami/airflow/dags
    - ./docker/requirements.txt:/bitnami/python/requirements.txt
  depends_on:
    - postgres
  healthcheck:
    &airflow-common-healthcheck
    start_period: 60s
    interval: 60s
    retries: 5

x-spark-common:
  &spark-common
  <<: *common
  image: bitnami/spark:3.5.5
  healthcheck:
    &spark-common-healthcheck
    <<: *common-healthcheck


services:
  postgres:
    <<: *common
    image: postgres:17.5
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      FLEET_DATABASE_PASSWORD: ${FLEET_DATABASE_PASSWORD:-fleet}
      AIRFLOW_DATABASE_PASSWORD: ${AIRFLOW_DATABASE_PASSWORD:-airflow}
      READER_DATABASE_PASSWORD: ${READER_DATABASE_PASSWORD:-reader}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/pg-init-user-db.sh:/docker-entrypoint-initdb.d/pg-init-user-db.sh
    ports:
      - '127.0.0.1:5432:5432'
    hostname: postgres
    healthcheck:
      <<: *common-healthcheck
      test: pg_isready -t 5 -h localhost -U postgres >/dev/null || exit 1

  clickhouse:
    <<: *common
    image: clickhouse:25.3.3.42
    environment:
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-default}
      FLEET_WAREHOUSE_PASSWORD: ${FLEET_WAREHOUSE_PASSWORD:-fleet}
      READER_WAREHOUSE_PASSWORD: ${READER_WAREHOUSE_PASSWORD:-reader}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./docker/ch-conf.xml:/etc/clickhouse-server/config.d/ch-conf.xml
      - ./docker/ch-init-user-db.sh:/docker-entrypoint-initdb.d/ch-init-user-db.sh
      - ./fleet/schema.sql:/opt/fleet/schema.sql
      - ./data:/var/lib/clickhouse/user_files
    ports:
      - '127.0.0.1:8123:8123'
      - '127.0.0.1:9000:9000'
    hostname: clickhouse
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1

  airflow-scheduler:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      AIRFLOW_COMPONENT_TYPE: scheduler
    hostname: airflow-scheduler
    healthcheck:
      <<: *airflow-common-healthcheck
      test: timeout 10 airflow jobs check --job-type SchedulerJob --local >/dev/null || exit 1

  airflow-dag-processor:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      AIRFLOW_COMPONENT_TYPE: dag-processor
    hostname: airflow-dag-processor
    healthcheck:
      <<: *airflow-common-healthcheck
      test: timeout 10 airflow jobs check --job-type DagProcessorJob --local >/dev/null || exit 1

  airflow:
    <<: *airflow-common
    environment:
      <<: *airflow-common-env
      AIRFLOW_COMPONENT_TYPE: api-server
      AIRFLOW_APISERVER_HOST: ${HOSTNAME:-localhost}
    ports:
      - '127.0.0.1:8080:8080'
    hostname: airflow
    healthcheck:
      <<: *airflow-common-healthcheck
      test: httpx --timeout 5 http://localhost:8080/api/v2/version >/dev/null || exit 1

  spark:
    <<: *spark-common
    environment:
      SPARK_MODE: master
    ports:
      - '127.0.0.1:8081:8080'
      - '127.0.0.1:7077:7077'
    hostname: spark
    healthcheck:
      <<: *spark-common-healthcheck
      test: timeout 5 bash -c "cat </dev/null >/dev/tcp/localhost/8080" || exit 1

  spark-worker:
    <<: *spark-common
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark:7077
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    scale: 2
    hostname: spark-worker
    healthcheck:
      <<: *spark-common-healthcheck
      test: timeout 5 bash -c "cat </dev/null >/dev/tcp/localhost/8081" || exit 1


volumes:
  postgres_data:
  clickhouse_data:
